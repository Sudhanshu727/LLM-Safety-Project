# 🛡️ LLM Safety Evaluation and Improvement

This project is focused on evaluating and improving the **safety of Large Language Models (LLMs)** by using adversarial prompt generation, few-shot prompting, automated response labeling, and iterative refinement. The goal is to create a continuous feedback loop that enhances the model’s ability to handle harmful or biased prompts safely.

---

## 📁 Project Structure
├── data/ # Collected prompts and responses
├── models/ # Saved classifier models
├── scripts/ # Scripts for generation, classification, mitigation
├── notebooks/ # Jupyter notebooks for testing or analysis
├── results/ # Graphs, evaluation results
├── docs/ # Report or supporting documents
└── README.md # Project description