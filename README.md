# ğŸ›¡ï¸ LLM Safety Evaluation and Improvement

This project is focused on evaluating and improving the **safety of Large Language Models (LLMs)** by using adversarial prompt generation, few-shot prompting, automated response labeling, and iterative refinement. The goal is to create a continuous feedback loop that enhances the modelâ€™s ability to handle harmful or biased prompts safely.

---

## ğŸ“ Project Structure

```
LLM Safety Project/
â”œâ”€â”€ script/
â”‚   â”œâ”€â”€ merge_labeled_data.py            # Merge new labels with the main dataset
â”‚   â”œâ”€â”€ auto_label_with_gemini.py        # Auto-label responses using Gemini API
â”‚   â”œâ”€â”€ generate_few_shot_prompt.py      # Create few-shot prompts for mitigation
â”‚   â”œâ”€â”€ generate_responses.py            # Generate initial model responses
â”‚   â””â”€â”€ groq_generate_with_few_shot.py   # Generate responses using few-shot prompting
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ labeled_responses.csv            # Full dataset of labeled model outputs
â”‚   â”œâ”€â”€ evaluated_red_team_results.csv   # Manually reviewed/annotated outputs
â”‚   â””â”€â”€ few_shot_prompt.txt              # Dynamically generated few-shot prompt
â”œâ”€â”€ results/
â”‚   â””â”€â”€ red_team_outputs.csv             # Raw model responses to adversarial prompts
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ (Documentation, reports, etc.)
â”œâ”€â”€ requirements.txt                     # Required Python packages
â””â”€â”€ README.md                            # Project overview and instructions
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/your-username/llm-safety-eval.git
cd llm-safety-eval/text
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set API Keys

Set your API keys as environment variables:

```bash
export GEMINI_API_KEY='your-gemini-api-key'
export GROQ_API_KEY='your-groq-api-key'
```

Alternatively, use a `.env` file and load it with a library like `python-dotenv`.

---

## ğŸ” Evaluation Workflow

Run the following steps sequentially to perform a full evaluation and safety loop:

1. **Generate model responses:**

```bash
python script/generate_responses.py
```

2. **Generate a new few-shot prompt from labeled data:**

```bash
python script/generate_few_shot_prompt.py
```

3. **Use Groq API with few-shot prompt to generate responses:**

```bash
python script/groq_generate_with_few_shot.py
```

4. **Auto-label responses using Gemini:**

```bash
python script/auto_label_with_gemini.py
```

5. **Merge newly labeled responses with master dataset:**

```bash
python script/merge_labeled_data.py
```

ğŸ”„ **Loop**: Go back to Step 2 and continue refining the few-shot prompt and responses for improved safety.

---

## âœ¨ Key Features

- **Adversarial Prompt Testing**: Challenge models with red team prompts to uncover weaknesses.
- **Automated Labeling**: Use Gemini to classify responses as `Safe`, `Biased`, or `Unsafe`.
- **Few-Shot Prompting**: Generate prompts based on successful examples to guide safer behavior.
- **Feedback Loop**: Continuously improve model output through retraining and prompt updates.
- **Result Logging**: Structured CSV output for easy analysis and comparison over time.

---

## ğŸ“„ Documentation

- All documentation and reports should be placed in the `docs/` folder.
- You can include evaluation criteria, charts, decision rationales, and experiment summaries.
- Track iterations of `few_shot_prompt.txt` and update `labeled_responses.csv` carefully for consistency and reproducibility.

---

## ğŸ¤ Contributions

We welcome contributions and new ideas related to:

- Prompt engineering
- Response evaluation techniques
- Safer output generation
- Integration with additional LLM APIs

Feel free to fork this repository and make it better!
