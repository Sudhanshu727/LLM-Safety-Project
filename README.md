# Safer LLMs â€“ AIMS-DTU Research Intern Project

## ğŸ§  Project Overview
This project explores how large language models (LLMs) can produce harmful, biased, or unsafe outputs, and implements methods to detect and mitigate such behavior.

## ğŸ“‹ Objectives
- Generate adversarial prompts and collect LLM outputs
- Classify responses as Safe / Biased / Unsafe
- Train a transformer-based classifier for safety detection
- Implement a mitigation method to reduce unsafe output

## ğŸ› ï¸ Tools & Libraries
- Model: LLaMA-2 Chat (via Hugging Face)
- Libraries: `transformers`, `huggingface_hub`, `scikit-learn`, `pandas`, `matplotlib`

## ğŸ“ Project Structure
â”œâ”€â”€ data/ # Collected prompts and responses
â”œâ”€â”€ models/ # Saved classifier models
â”œâ”€â”€ scripts/ # Scripts for generation, classification, mitigation
â”œâ”€â”€ notebooks/ # Jupyter notebooks for testing or analysis
â”œâ”€â”€ results/ # Graphs, evaluation results
â”œâ”€â”€ docs/ # Report or supporting documents
â””â”€â”€ README.md # Project description