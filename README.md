# ğŸ›¡ï¸ LLM Safety Evaluation and Improvement

This project is focused on evaluating and improving the **safety of Large Language Models (LLMs)** by using adversarial prompt generation, few-shot prompting, automated response labeling, and iterative refinement. The goal is to create a continuous feedback loop that enhances the modelâ€™s ability to handle harmful or biased prompts safely.

---

## ğŸ“ Project Structure
â”œâ”€â”€ data/ # Collected prompts and responses
â”œâ”€â”€ models/ # Saved classifier models
â”œâ”€â”€ scripts/ # Scripts for generation, classification, mitigation
â”œâ”€â”€ notebooks/ # Jupyter notebooks for testing or analysis
â”œâ”€â”€ results/ # Graphs, evaluation results
â”œâ”€â”€ docs/ # Report or supporting documents
â””â”€â”€ README.md # Project description